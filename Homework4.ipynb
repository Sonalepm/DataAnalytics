{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 1: Smart Job Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib\n",
    "import html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL\n",
      "\n",
      "  * new york >\n",
      "\n",
      "  * manhattan >\n",
      "\n",
      "  * jobs >\n",
      "\n",
      "  * software/qa/dba/etc\n",
      "\n",
      "  * post\n",
      "  * account\n",
      "\n",
      "  * favorites\n",
      "\n",
      "  * hidden\n",
      "\n",
      "CL\n",
      "\n",
      "manhattan > software/qa/dba/etc\n",
      "\n",
      "...\n",
      "\n",
      "◀ prev  ▲ next ▶\n",
      "\n",
      "reply\n",
      "\n",
      "favorite\n",
      "\n",
      "favorite\n",
      "\n",
      "hide\n",
      "\n",
      "unhide\n",
      "\n",
      "⚐ ⚑\n",
      "\n",
      "flag\n",
      "\n",
      "⚑\n",
      "\n",
      "flagged\n",
      "\n",
      "Posted  2019-10-27 09:41\n",
      "\n",
      "Contact Information:\n",
      "\n",
      "print\n",
      "\n",
      "##  Python whiz -- nerds welcome! (Union Square)\n",
      "\n",
      "(google map)\n",
      "\n",
      "compensation: **Competitive and commensurate with experience**  \n",
      "employment type: **employee 's choice**  \n",
      "\n",
      "QR Code Link to This Post\n",
      "\n",
      "Do you like spreadsheets? Data? Classifying and organizing complex\n",
      "information? If so, we want to meet you. We are an established-but-still-\n",
      "unconventional 25-person technology company in the digital health space. We\n",
      "have need of a highly organized, clear-thinking researcher to locate, gather,\n",
      "and classify large amounts of information. You must be extremely proficient in\n",
      "Python. If you have other software engineering skills (or interests), that's a\n",
      "plus. Role can be full or part time, depending upon the preferences of the\n",
      "right candidate. Join a collaborative, creative, committed team of internet\n",
      "industry veterans. We look forward to hearing from you.\n",
      "\n",
      "  * Principals only. Recruiters, please don't contact this job poster.\n",
      "  * do NOT contact us with unsolicited services or offers\n",
      "\n",
      "post id: 7008014434\n",
      "\n",
      "posted: 2019-10-27 09:41\n",
      "\n",
      "email to friend\n",
      "\n",
      "♥ best of [?]\n",
      "\n",
      "  * (C) craigslistCL\n",
      "  * help\n",
      "  * safety\n",
      "  * privacy\n",
      "  * feedback\n",
      "  * cl jobs\n",
      "  * termsnew\n",
      "  * about\n",
      "  * mobile\n",
      "  * desktop\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resource = urllib.request.urlopen(\"https://newyork.craigslist.org/mnh/sof/d/new-york-city-python-whiz-nerds-welcome/7008014434.html\")\n",
    "content = resource.read()\n",
    "charset = resource.headers.get_content_charset()\n",
    "content = content.decode(charset)\n",
    "print(h.handle(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobdesc = h.handle(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting job content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descr = re.search(r'Posted(.*\\n)+posted: [0-9]{4,4}-[0-9]{1,2}-[0-9]{1,2} [0-9]{1,2}:[0-9]{1,2}',jobdesc,re.M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = job_descr.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Posted  2019-10-27 09:41\\n\\nContact Information:\\n\\nprint\\n\\n##  Python whiz -- nerds welcome! (Union Square)\\n\\n(google map)\\n\\ncompensation: **Competitive and commensurate with experience**  \\nemployment type: **employee 's choice**  \\n\\nQR Code Link to This Post\\n\\nDo you like spreadsheets? Data? Classifying and organizing complex\\ninformation? If so, we want to meet you. We are an established-but-still-\\nunconventional 25-person technology company in the digital health space. We\\nhave need of a highly organized, clear-thinking researcher to locate, gather,\\nand classify large amounts of information. You must be extremely proficient in\\nPython. If you have other software engineering skills (or interests), that's a\\nplus. Role can be full or part time, depending upon the preferences of the\\nright candidate. Join a collaborative, creative, committed team of internet\\nindustry veterans. We look forward to hearing from you.\\n\\n  * Principals only. Recruiters, please don't contact this job poster.\\n  * do NOT contact us with unsolicited services or offers\\n\\npost id: 7008014434\\n\\nposted: 2019-10-27 09:41\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Posted Contact Information print Python whiz nerds welcome Union Square google map compensation Competitive and commensurate with experience employment type employee s choice QR Code Link to This Post Do you like spreadsheets Data Classifying and organizing complex information If so we want to meet you We are an established but still unconventional person technology company in the digital health space We have need of a highly organized clear thinking researcher to locate gather and classify large amounts of information You must be extremely proficient in Python If you have other software engineering skills or interests that s a plus Role can be full or part time depending upon the preferences of the right candidate Join a collaborative creative committed team of internet industry veterans We look forward to hearing from you Principals only Recruiters please don t contact this job poster do NOT contact us with unsolicited services or offers post id posted '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pro = re.sub('[^A-Za-z]+', ' ', data)\n",
    "data_pro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forming bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading NLTK\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Posted', 'Contact', 'Information', 'print', 'Python', 'whiz', 'nerds', 'welcome', 'Union', 'Square', 'google', 'map', 'compensation', 'Competitive', 'and', 'commensurate', 'with', 'experience', 'employment', 'type', 'employee', 's', 'choice', 'QR', 'Code', 'Link', 'to', 'This', 'Post', 'Do', 'you', 'like', 'spreadsheets', 'Data', 'Classifying', 'and', 'organizing', 'complex', 'information', 'If', 'so', 'we', 'want', 'to', 'meet', 'you', 'We', 'are', 'an', 'established', 'but', 'still', 'unconventional', 'person', 'technology', 'company', 'in', 'the', 'digital', 'health', 'space', 'We', 'have', 'need', 'of', 'a', 'highly', 'organized', 'clear', 'thinking', 'researcher', 'to', 'locate', 'gather', 'and', 'classify', 'large', 'amounts', 'of', 'information', 'You', 'must', 'be', 'extremely', 'proficient', 'in', 'Python', 'If', 'you', 'have', 'other', 'software', 'engineering', 'skills', 'or', 'interests', 'that', 's', 'a', 'plus', 'Role', 'can', 'be', 'full', 'or', 'part', 'time', 'depending', 'upon', 'the', 'preferences', 'of', 'the', 'right', 'candidate', 'Join', 'a', 'collaborative', 'creative', 'committed', 'team', 'of', 'internet', 'industry', 'veterans', 'We', 'look', 'forward', 'to', 'hearing', 'from', 'you', 'Principals', 'only', 'Recruiters', 'please', 'don', 't', 'contact', 'this', 'job', 'poster', 'do', 'NOT', 'contact', 'us', 'with', 'unsolicited', 'services', 'or', 'offers', 'post', 'id', 'posted']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(data_pro)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sonale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'don', 'm', 'against', 'whom', 'those', 'what', 'such', 'theirs', 'until', 'him', 'yours', 'hers', 'shouldn', \"she's\", 'doing', 'they', 'further', 'be', \"it's\", \"shan't\", 'off', 'she', 'won', \"wasn't\", 'then', 'herself', \"you're\", 'only', 'once', 'over', 'needn', 'why', \"couldn't\", 'do', 'these', 'its', 'i', 'd', 'into', \"wouldn't\", 'their', 'the', 'that', 'had', 'with', 'but', 'more', 'all', \"haven't\", 'same', 'have', 'isn', 'just', 'and', 'll', 'because', 't', 'very', 'my', \"mightn't\", \"hadn't\", \"hasn't\", 'is', 'down', 'who', 'how', 'haven', 'ma', 'here', 'should', 'myself', 'in', 'than', 'on', 'he', 'so', \"weren't\", 'having', 'between', 'itself', 'there', 'after', 'each', 's', 'wasn', 'ours', 'any', 'y', 'being', 'mightn', \"that'll\", \"you'd\", \"you'll\", 'can', 've', 'some', 'few', 'ain', 'for', 'under', 'hadn', 'up', 'now', 'wouldn', 'yourselves', 'has', 'will', 'Posted', 'about', 'as', 'to', 'ourselves', 'her', 'again', 'other', 'an', 'own', 'doesn', \"shouldn't\", 'nor', 'weren', 'which', 'below', 'while', 'out', 'themselves', 'during', 'his', 'didn', \"needn't\", 'it', 'at', 'from', 'no', 'o', 'you', 'were', 'your', 'this', 'does', \"don't\", 'posted', 'yourself', 'where', 'above', 'or', \"won't\", 'not', 'our', 'when', 'by', \"isn't\", \"mustn't\", 'was', 'g', \"aren't\", 'are', 'me', 'been', 'couldn', 'hasn', 'shan', 'we', 'if', \"you've\", 'aren', 'himself', 'did', 'of', 'too', 'them', 'am', 'mustn', 'both', \"doesn't\", 'before', 'through', 'most', 'a', 're', \"should've\", \"didn't\"}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.add('posted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.add('Posted')\n",
    "stop_words.add('We')\n",
    "stop_words.add('You')\n",
    "stop_words.add('If')\n",
    "stop_words.add('please')\n",
    "stop_words.add('print')\n",
    "stop_words.add('QR')\n",
    "stop_words.add('NOT')\n",
    "stop_words.add('contact')\n",
    "stop_words.add('look')\n",
    "stop_words.add('forward')\n",
    "stop_words.add('hearing')\n",
    "stop_words.add('us')\n",
    "stop_words.add('post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filterd Sentence: ['Contact', 'Information', 'Python', 'whiz', 'nerds', 'welcome', 'Union', 'Square', 'google', 'map', 'compensation', 'Competitive', 'commensurate', 'experience', 'employment', 'type', 'employee', 'choice', 'Code', 'Link', 'This', 'Post', 'Do', 'like', 'spreadsheets', 'Data', 'Classifying', 'organizing', 'complex', 'information', 'want', 'meet', 'established', 'still', 'unconventional', 'person', 'technology', 'company', 'digital', 'health', 'space', 'need', 'highly', 'organized', 'clear', 'thinking', 'researcher', 'locate', 'gather', 'classify', 'large', 'amounts', 'information', 'must', 'extremely', 'proficient', 'Python', 'software', 'engineering', 'skills', 'interests', 'plus', 'Role', 'full', 'part', 'time', 'depending', 'upon', 'preferences', 'right', 'candidate', 'Join', 'collaborative', 'creative', 'committed', 'team', 'internet', 'industry', 'veterans', 'Principals', 'Recruiters', 'job', 'poster', 'unsolicited', 'services', 'offers', 'id']\n"
     ]
    }
   ],
   "source": [
    "filtered_word=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_word.append(w)\n",
    "print(\"Filterd Sentence:\",filtered_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(filtered_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Python': 2, 'information': 2, 'Contact': 1, 'Information': 1, 'whiz': 1, 'nerds': 1, 'welcome': 1, 'Union': 1, 'Square': 1, 'google': 1, ...})"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize words: ['Contact', 'Information', 'Python', 'whiz', 'nerd', 'welcome', 'Union', 'Square', 'google', 'map', 'compensation', 'Competitive', 'commensurate', 'experience', 'employment', 'type', 'employee', 'choice', 'Code', 'Link', 'This', 'Post', 'Do', 'like', 'spreadsheet', 'Data', 'Classifying', 'organizing', 'complex', 'information', 'want', 'meet', 'established', 'still', 'unconventional', 'person', 'technology', 'company', 'digital', 'health', 'space', 'need', 'highly', 'organized', 'clear', 'thinking', 'researcher', 'locate', 'gather', 'classify', 'large', 'amount', 'information', 'must', 'extremely', 'proficient', 'Python', 'software', 'engineering', 'skill', 'interest', 'plus', 'Role', 'full', 'part', 'time', 'depending', 'upon', 'preference', 'right', 'candidate', 'Join', 'collaborative', 'creative', 'committed', 'team', 'internet', 'industry', 'veteran', 'Principals', 'Recruiters', 'job', 'poster', 'unsolicited', 'service', 'offer', 'id']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "lem_words=[]\n",
    "for w in filtered_word:\n",
    "    lem_words.append(lem.lemmatize(w))\n",
    "\n",
    "#print(\"Filtered words:\",filtered_word)\n",
    "print(\"Lemmatize words:\",lem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read resume and lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readdoc(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    \n",
    "    resumeContent = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        resumeContent.append(paragraph.text)\n",
    "    return '\\n'.join(resumeContent)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = readdoc('Sonale_PM-Resume.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sonale Palliparangattu Mullepattu\n",
      "Iselin, New Jersey • 732-715-7094 • \n",
      "LinkedIn: \n",
      "Dedicated and innovative Application Developer with two plus years professional background in database administration and innovative technology solutions across current and emergent industries. Effective team player with expertise across broad spectrum while seeing projects from infancy to production and implementation. \n",
      "Academic Details\n",
      "Master of Science in Computer Science – Expected Graduation  Dec 2019, GPA – 4.0/4.0\n",
      "New Jersey Institute of Technology, Newark, USA\n",
      "Bachelor of Science in Electronics and Communication- 2013\n",
      "Cochin University of Science and Technology, Kerala, India\n",
      "\n",
      "TECHNICAL SKILLS:\n",
      "Databases\t  Oracle 9i, 10g,11g,12c, Informix IDS 9.2, MySQL, MongoDB atlas\n",
      "Languages\tSQL, PL/SQL, C, Java, PHP, Python, HTML, CSS, Shell scripting\n",
      "Webserver                                                                  XAMPP, Apache Tomcat\n",
      "Bigdata Technologies                                               Hive, Pig, Cloudera Hadoop eco system\n",
      "Tools | Platforms | Frameworks                          Eclipse, MySQL workbench, Spring MVC, SQL Developer, Sublime, Atom, phpMyAdmin, Jira, Wireshark, Git, AWS, IBM Cloud, Morphia, Junit, Hamcrest, Weka: Machine Learning suite\n",
      "\n",
      "\n",
      "Work Experience Highlights\n",
      "FORBES MEDIA | Backend Engineering Intern, June- August 2019\n",
      "Develop API/endpoints for frontend applications to interact with MongoDB.\n",
      "Worked on Jira tickets for adding image credit to gallery data, pagination of articles written on Forbes internal CMS.\n",
      "Experience working in an agile environment.\n",
      "NJIT | MS Grader, Feb- May 2019\n",
      "Assist in teaching and grading for two sections of undergraduate courses on Database design\n",
      "IBM | Client: Blue Cross Blue Shield | Oracle DBA & Application Developer, 2014-2016\n",
      "Designed, developed and implemented logical and physical data model for existing and new applications. \n",
      "Acted as single point of contact for critical health care applications. Co-ordinated with offshore and onsite teams for  defect resolution.\n",
      "Performed data analysis and requirement gathering. Maintained SRS Baseline, development, quality reviews, ST/UAT and Production support.\n",
      "Created UNIX shell scripts, ESQL, SQLs, PL/SQL procedures, functions, packages ,triggers and materialized views to support and automate repetitive tasks.\n",
      "Introduced improvements and changes to the production database and linked databases for data retrieval and management. \n",
      "Professional Awards & Trainings\n",
      "\n",
      "IBM Managers Choice Award 2014, 2015 for \"Putting the Client First\"\n",
      "Oracle Database 11g: SQL Fundamentals I Certified\n",
      "IBM CE- Foundation Course in Big Data Analytics\n",
      "IBM Corporate Training in RDBMS, UNIX, Pl/Sql\n",
      "Big Data University Hadoop 101, Big Data 101 Certified\n",
      "Udemy certification on Command line essentials: Git Bash \n",
      "BSNL Industrial Training in Telecom Technology\n",
      "Projects \n",
      "Florence Chatbot | December 2018 – January 2019\n",
      "Developed a chatbot using IBM Watson Assistant for a flower shop to understand and help customers. \n",
      "Technologies Used : IBM Cloud , Watson Assistant \n",
      "TIJN Payment portal | September 2018 – December 2018 \n",
      "Designed and developed a web application that enables users to electronically transfer money. \n",
      "Technologies Used : MySQL database, XAMPP webserver, HTML,CSS,PHP for front end and backend coding\n",
      "Home Automation using Raspberry Pi | November – December 2018 \n",
      "Developed interactive home automation system using RPi3b+ which enables users to control devices via web interface.\n",
      "Technologies Used : MySQL database, Python , Raspberry Pi 3b+, Apache Tomcat \n",
      "Classification of Breast Cancer data | July – August 2019\n",
      "Use Naïve Bayes, Decision Tree Algorithms to classify, label breast cancer data from Wisconsin Diagnostic dataset.\n",
      "Technologies Used : Weka Machine Learning Suite \n"
     ]
    }
   ],
   "source": [
    "print(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TECHNICAL SKILLS:\n",
      "Databases\t  Oracle 9i, 10g,11g,12c, Informix IDS 9.2, MySQL, MongoDB atlas\n",
      "Languages\tSQL, PL/SQL, C, Java, PHP, Python, HTML, CSS, Shell scripting\n",
      "Webserver                                                                  XAMPP, Apache Tomcat\n",
      "Bigdata Technologies                                               Hive, Pig, Cloudera Hadoop eco system\n",
      "Tools | Platforms | Frameworks                          Eclipse, MySQL workbench, Spring MVC, SQL Developer, Sublime, Atom, phpMyAdmin, Jira, Wireshark, Git, AWS, IBM Cloud, Morphia, Junit, Hamcrest, Weka: Machine Learning suite\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resume_cont = re.search(r'TECHNICAL SKILLS:(.*\\n)+\\n\\n',resume,re.M)\n",
    "print(resume_cont.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dedicated and innovative Application Developer with two plus years professional background in database administration and innovative technology solutions across current and emergent industries. Effective team player with expertise across broad spectrum while seeing projects from infancy to production and implementation. \n",
      "Academic Details\n",
      "Master of Science in Computer Science – Expected Graduation  Dec 2019, GPA – 4.0/4.0\n",
      "New Jersey Institute of Technology, Newark, USA\n",
      "Bachelor of Science in Electronics and Communication- 2013\n",
      "Cochin University of Science and Technology, Kerala, India\n",
      "\n",
      "TECHNICAL SKILLS:\n",
      "Databases\t  Oracle 9i, 10g,11g,12c, Informix IDS 9.2, MySQL, MongoDB atlas\n",
      "Languages\tSQL, PL/SQL, C, Java, PHP, Python, HTML, CSS, Shell scripting\n",
      "Webserver                                                                  XAMPP, Apache Tomcat\n",
      "Bigdata Technologies                                               Hive, Pig, Cloudera Hadoop eco system\n",
      "Tools | Platforms | Frameworks                          Eclipse, MySQL workbench, Spring MVC, SQL Developer, Sublime, Atom, phpMyAdmin, Jira, Wireshark, Git, AWS, IBM Cloud, Morphia, Junit, Hamcrest, Weka: Machine Learning suite\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resume_content = re.search(r'Dedicated(.*\\n)+\\n\\n',resume,re.M)\n",
    "print(resume_content.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cont = resume_content.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dedicated and innovative Application Developer with two plus years professional background in database administration and innovative technology solutions across current and emergent industries Effective team player with expertise across broad spectrum while seeing projects from infancy to production and implementation Academic Details Master of Science in Computer Science Expected Graduation Dec GPA New Jersey Institute of Technology Newark USA Bachelor of Science in Electronics and Communication Cochin University of Science and Technology Kerala India TECHNICAL SKILLS Databases Oracle i g g c Informix IDS MySQL MongoDB atlas Languages SQL PL SQL C Java PHP Python HTML CSS Shell scripting Webserver XAMPP Apache Tomcat Bigdata Technologies Hive Pig Cloudera Hadoop eco system Tools Platforms Frameworks Eclipse MySQL workbench Spring MVC SQL Developer Sublime Atom phpMyAdmin Jira Wireshark Git AWS IBM Cloud Morphia Junit Hamcrest Weka Machine Learning suite '"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_pro = re.sub('[^A-Za-z]+', ' ', res_cont)\n",
    "res_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dedicated', 'and', 'innovative', 'Application', 'Developer', 'with', 'two', 'plus', 'years', 'professional', 'background', 'in', 'database', 'administration', 'and', 'innovative', 'technology', 'solutions', 'across', 'current', 'and', 'emergent', 'industries', 'Effective', 'team', 'player', 'with', 'expertise', 'across', 'broad', 'spectrum', 'while', 'seeing', 'projects', 'from', 'infancy', 'to', 'production', 'and', 'implementation', 'Academic', 'Details', 'Master', 'of', 'Science', 'in', 'Computer', 'Science', 'Expected', 'Graduation', 'Dec', 'GPA', 'New', 'Jersey', 'Institute', 'of', 'Technology', 'Newark', 'USA', 'Bachelor', 'of', 'Science', 'in', 'Electronics', 'and', 'Communication', 'Cochin', 'University', 'of', 'Science', 'and', 'Technology', 'Kerala', 'India', 'TECHNICAL', 'SKILLS', 'Databases', 'Oracle', 'i', 'g', 'g', 'c', 'Informix', 'IDS', 'MySQL', 'MongoDB', 'atlas', 'Languages', 'SQL', 'PL', 'SQL', 'C', 'Java', 'PHP', 'Python', 'HTML', 'CSS', 'Shell', 'scripting', 'Webserver', 'XAMPP', 'Apache', 'Tomcat', 'Bigdata', 'Technologies', 'Hive', 'Pig', 'Cloudera', 'Hadoop', 'eco', 'system', 'Tools', 'Platforms', 'Frameworks', 'Eclipse', 'MySQL', 'workbench', 'Spring', 'MVC', 'SQL', 'Developer', 'Sublime', 'Atom', 'phpMyAdmin', 'Jira', 'Wireshark', 'Git', 'AWS', 'IBM', 'Cloud', 'Morphia', 'Junit', 'Hamcrest', 'Weka', 'Machine', 'Learning', 'suite']\n"
     ]
    }
   ],
   "source": [
    "tokenized_res=word_tokenize(res_pro)\n",
    "print(tokenized_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.add('g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filterd words: ['Dedicated', 'innovative', 'Application', 'Developer', 'two', 'plus', 'years', 'professional', 'background', 'database', 'administration', 'innovative', 'technology', 'solutions', 'across', 'current', 'emergent', 'industries', 'Effective', 'team', 'player', 'expertise', 'across', 'broad', 'spectrum', 'seeing', 'projects', 'infancy', 'production', 'implementation', 'Academic', 'Details', 'Master', 'Science', 'Computer', 'Science', 'Expected', 'Graduation', 'Dec', 'GPA', 'New', 'Jersey', 'Institute', 'Technology', 'Newark', 'USA', 'Bachelor', 'Science', 'Electronics', 'Communication', 'Cochin', 'University', 'Science', 'Technology', 'Kerala', 'India', 'TECHNICAL', 'SKILLS', 'Databases', 'Oracle', 'c', 'Informix', 'IDS', 'MySQL', 'MongoDB', 'atlas', 'Languages', 'SQL', 'PL', 'SQL', 'C', 'Java', 'PHP', 'Python', 'HTML', 'CSS', 'Shell', 'scripting', 'Webserver', 'XAMPP', 'Apache', 'Tomcat', 'Bigdata', 'Technologies', 'Hive', 'Pig', 'Cloudera', 'Hadoop', 'eco', 'system', 'Tools', 'Platforms', 'Frameworks', 'Eclipse', 'MySQL', 'workbench', 'Spring', 'MVC', 'SQL', 'Developer', 'Sublime', 'Atom', 'phpMyAdmin', 'Jira', 'Wireshark', 'Git', 'AWS', 'IBM', 'Cloud', 'Morphia', 'Junit', 'Hamcrest', 'Weka', 'Machine', 'Learning', 'suite']\n"
     ]
    }
   ],
   "source": [
    "filtered_res=[]\n",
    "for w in tokenized_res:\n",
    "    if w not in stop_words:\n",
    "        filtered_res.append(w)\n",
    "#print(\"Tokenized Sentence:\",tokenized_word)\n",
    "print(\"Filterd words:\",filtered_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Science': 4, 'SQL': 3, 'innovative': 2, 'Developer': 2, 'across': 2, 'Technology': 2, 'MySQL': 2, 'Dedicated': 1, 'Application': 1, 'two': 1, ...})"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_res = FreqDist(filtered_res)\n",
    "fdist_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize words: ['Dedicated', 'innovative', 'Application', 'Developer', 'two', 'plus', 'year', 'professional', 'background', 'database', 'administration', 'innovative', 'technology', 'solution', 'across', 'current', 'emergent', 'industry', 'Effective', 'team', 'player', 'expertise', 'across', 'broad', 'spectrum', 'seeing', 'project', 'infancy', 'production', 'implementation', 'Academic', 'Details', 'Master', 'Science', 'Computer', 'Science', 'Expected', 'Graduation', 'Dec', 'GPA', 'New', 'Jersey', 'Institute', 'Technology', 'Newark', 'USA', 'Bachelor', 'Science', 'Electronics', 'Communication', 'Cochin', 'University', 'Science', 'Technology', 'Kerala', 'India', 'TECHNICAL', 'SKILLS', 'Databases', 'Oracle', 'c', 'Informix', 'IDS', 'MySQL', 'MongoDB', 'atlas', 'Languages', 'SQL', 'PL', 'SQL', 'C', 'Java', 'PHP', 'Python', 'HTML', 'CSS', 'Shell', 'scripting', 'Webserver', 'XAMPP', 'Apache', 'Tomcat', 'Bigdata', 'Technologies', 'Hive', 'Pig', 'Cloudera', 'Hadoop', 'eco', 'system', 'Tools', 'Platforms', 'Frameworks', 'Eclipse', 'MySQL', 'workbench', 'Spring', 'MVC', 'SQL', 'Developer', 'Sublime', 'Atom', 'phpMyAdmin', 'Jira', 'Wireshark', 'Git', 'AWS', 'IBM', 'Cloud', 'Morphia', 'Junit', 'Hamcrest', 'Weka', 'Machine', 'Learning', 'suite']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lem_res=[]\n",
    "for w in filtered_res:\n",
    "    lem_res.append(lem.lemmatize(w))\n",
    "\n",
    "#print(\"Filtered words:\",filtered_word)\n",
    "print(\"Lemmatize words:\",lem_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lem_res - lemmatized words from resume\n",
    "#lem_words - lemmatized words from job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [item.lower() for item in lem_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_res = pd.DataFrame(l,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_proc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dedicated</td>\n",
       "      <td>dedicate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>innovative</td>\n",
       "      <td>innovative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>application</td>\n",
       "      <td>application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>developer</td>\n",
       "      <td>developer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text    text_proc\n",
       "0    dedicated     dedicate\n",
       "1   innovative   innovative\n",
       "2  application  application\n",
       "3    developer    developer\n",
       "4          two          two"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_res['text_proc']=data_res['text'].apply(preprocessing)\n",
    "data_res.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_df = pd.DataFrame.from_dict(fdist,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.sort_values(by=[0,'index'], ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus.reader.wordnet import NOUN\n",
    "from nltk.corpus.reader.wordnet import VERB\n",
    "from nltk.corpus.reader.wordnet import ADJ\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(text):\n",
    "    wordset_n = set(lemmatizer.lemmatize(w, NOUN) for w in word_tokenize(text.lower().strip()))\n",
    "    #print(wordset_n)\n",
    "    wordset_v = set(lemmatizer.lemmatize(w, VERB) for w in wordset_n)\n",
    "    \n",
    "    wordset = set(lemmatizer.lemmatize(w, ADJ) for w in wordset_v)\n",
    "    #print(wordset)\n",
    "    #print(worddict)\n",
    "    wordset = wordset\n",
    "    return ' '.join(list(wordset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>information</td>\n",
       "      <td>2</td>\n",
       "      <td>information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python</td>\n",
       "      <td>2</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whiz</td>\n",
       "      <td>1</td>\n",
       "      <td>whiz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>welcome</td>\n",
       "      <td>1</td>\n",
       "      <td>welcome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "      <td>want</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  0         text\n",
       "29  information  2  information\n",
       "2        Python  2       python\n",
       "3          whiz  1         whiz\n",
       "5       welcome  1      welcome\n",
       "30         want  1         want"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['text']=data_df['index'].apply(preprocessing)\n",
    "data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = list(data_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['information',\n",
       " 'python',\n",
       " 'whiz',\n",
       " 'welcome',\n",
       " 'want',\n",
       " 'veteran',\n",
       " 'upon',\n",
       " 'unsolicited',\n",
       " 'unconventional',\n",
       " 'type',\n",
       " 'time',\n",
       " 'think',\n",
       " 'technology',\n",
       " 'team',\n",
       " 'still',\n",
       " 'spreadsheet',\n",
       " 'space',\n",
       " 'software',\n",
       " 'skill',\n",
       " 'service',\n",
       " 'right',\n",
       " 'researcher',\n",
       " 'proficient',\n",
       " 'preference',\n",
       " 'poster',\n",
       " 'plus',\n",
       " 'person',\n",
       " 'part',\n",
       " 'organize',\n",
       " 'organize',\n",
       " 'offer',\n",
       " 'nerd',\n",
       " 'need',\n",
       " 'must',\n",
       " 'meet',\n",
       " 'map',\n",
       " 'locate',\n",
       " 'like',\n",
       " 'large',\n",
       " 'job',\n",
       " 'internet',\n",
       " 'interest',\n",
       " 'industry',\n",
       " 'id',\n",
       " 'highly',\n",
       " 'health',\n",
       " 'google',\n",
       " 'gather',\n",
       " 'full',\n",
       " 'extremely',\n",
       " 'experience',\n",
       " 'establish',\n",
       " 'engineer',\n",
       " 'employment',\n",
       " 'employee',\n",
       " 'digital',\n",
       " 'depend',\n",
       " 'creative',\n",
       " 'complex',\n",
       " 'compensation',\n",
       " 'company',\n",
       " 'commit',\n",
       " 'commensurate',\n",
       " 'collaborative',\n",
       " 'clear',\n",
       " 'classify',\n",
       " 'choice',\n",
       " 'candidate',\n",
       " 'amount',\n",
       " 'union',\n",
       " 'this',\n",
       " 'square',\n",
       " 'role',\n",
       " 'recruiter',\n",
       " 'principal',\n",
       " 'post',\n",
       " 'link',\n",
       " 'join',\n",
       " 'information',\n",
       " 'do',\n",
       " 'data',\n",
       " 'contact',\n",
       " 'competitive',\n",
       " 'code',\n",
       " 'classify']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['python','code','data','classify','software','skill','think','technology','proficient','spreadsheet','team','organize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = [1 if v in doc else 0 for v in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bernoulli [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print('bernoulli', bernoulli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial = [doc.count(v) for v in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomial [1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "print('multinomial', multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = list(data_res['text_proc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomial2 [1, 0, 0, 0, 0, 1, 0, 4, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "multinomial2 = [doc2.count(v) for v in words]\n",
    "print('multinomial2', multinomial2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    import math\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3785166493051126"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(multinomial, multinomial2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
